{"tool_name":"selfat-fold","contents":["biotools","bioschemas"],"fetched_metadata":{"biotools":{"id":"selfat-fold","home":"http://bliulab.net/selfAT_fold/","summary":"protein fold recognition based on residue-based and motif-based self-attention networks.\n\n1. The similarities between the self-attention mechanism for natural language processing (a) and the self-attention mechanism based on the structure motifs for protein fold recognition (b). The lines represent the attention weights. The darker the line is, the higher the corre-sponding attention weight is.\n\n2. The flowchart of the motif-based self-attention network (MSAN) for extracting fold-specific attention features. (a) the motif convolution layer, (b) the self-attention layer, and (c) the full connected layer.","addition_date":"2021-01-18T11:29:56Z","last_update_date":"2021-02-16T14:07:56Z"},"bioschemas":{"name":"SelfAT-Fold","home":"https://bio.tools/selfat-fold","summary":"protein fold recognition based on residue-based and motif-based self-attention networks.\n\n1. The similarities between the self-attention mechanism for natural language processing (a) and the self-attention mechanism based on the structure motifs for protein fold recognition (b). The lines represent the attention weights. The darker the line is, the higher the corre-sponding attention weight is.\n\n2. The flowchart of the motif-based self-attention network (MSAN) for extracting fold-specific attention features. (a) the motif convolution layer, (b) the self-attention layer, and (c) the full connected layer.","tool_type":"sc:SoftwareApplication"}}}