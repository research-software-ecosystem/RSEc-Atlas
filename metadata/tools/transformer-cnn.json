{"tool_name":"transformer-cnn","contents":["biotools","bioschemas"],"fetched_metadata":{"biotools":{"id":"Transformer-CNN","home":"https://github.com/bigchem/transformer-cnn","summary":"Fast and Reliable Tool for QSAR.\n\nTransformer CNN for QSAR/QSPR modelling.\n\nThe repository contains the source code for a new Transformer-CNN method described in our paper http://arxiv.org/abs/1911.06603. First, we trained the Transformer model on SMILES canonicalization task, e.g., given an arbitrary SMILES, the model converts it to a canonical one. Second, we use the internal representation of the Transformer (the output of the encoding stack with shape (BATCH, LENGTH, EMBEDDING)) as SMILES embeddings and build upon them CharNN model (Convolution and HighWay as it is done in DeepChem). The resulting model works both in classification and regression settings","addition_date":"2020-01-09T17:42:55Z","last_update_date":"2020-12-30T15:06:17Z"},"bioschemas":{"name":"Transformer-CNN","home":"https://bio.tools/Transformer-CNN","summary":"Fast and Reliable Tool for QSAR.\n\nTransformer CNN for QSAR/QSPR modelling.\n\nThe repository contains the source code for a new Transformer-CNN method described in our paper http://arxiv.org/abs/1911.06603. First, we trained the Transformer model on SMILES canonicalization task, e.g., given an arbitrary SMILES, the model converts it to a canonical one. Second, we use the internal representation of the Transformer (the output of the encoding stack with shape (BATCH, LENGTH, EMBEDDING)) as SMILES embeddings and build upon them CharNN model (Convolution and HighWay as it is done in DeepChem). The resulting model works both in classification and regression settings","tool_type":"sc:SoftwareApplication"}}}